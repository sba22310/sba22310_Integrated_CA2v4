{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de951a2",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c60d1",
   "metadata": {},
   "source": [
    "Importing libraries, starting session and reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac2e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing necessary libraries\n",
    "\n",
    "#General\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "#Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, TimestampType, FloatType\n",
    "\n",
    "#Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Preprocessing data\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.correlation import plot_corr, plot_corr_grid\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab16598",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Startig Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Greta\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 500) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Increasing available memory for Spark\n",
    "spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "spark.conf.set(\"spark.executor.memory\",\"4g\")\n",
    "spark.conf.set(\"spark.driver.memory\",\"4g\")\n",
    "spark.conf.set(\"spark.driver.maxResultSize\",\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read JSON file into dataframe\n",
    "df = spark.read.json(\"hdfs://localhost:9000/ca2/Greta/greta.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11a764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Printing schema and showing\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83097dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting numer of rows (tweets)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing 1st axis columns\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef559429",
   "metadata": {},
   "source": [
    "For univariate analysis timestamp and text of tweets are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting only necessary columns and displaying for review\n",
    "df.select(\"created_at\",\"full_text\").show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18abe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Displaying full text for additional review\n",
    "df.select(\"created_at\",\"full_text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f6268",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922d31e",
   "metadata": {},
   "source": [
    "Fixing timestamps, cleaning text, reducing data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006be9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating new df from selected columns\n",
    "df_work = df.select(\"created_at\",\"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating function for cleaning texts\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'greta', ' ', text)\n",
    "    text = re.sub(r'thunberg', ' ', text)\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', ' ', text)   \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', ' ', text)   \n",
    "    text = re.sub(r'www.[^ ]+', '', text)  \n",
    "    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', ' ', text)  \n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = [token for token in text.split() if len(token) > 2]\n",
    "    text = [token for token in text if token]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating UDF with function for cleaning text to be applied on the column\n",
    "cleanUDF = udf(lambda x:clean_text(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying cleanUDF on new column\n",
    "df_work = df_work.withColumn('Text', cleanUDF(F.col('full_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating function for reshaping timestamp\n",
    "def createTimestamp(created_at):\n",
    "    newTimestamp = datetime.strftime(datetime.strptime\n",
    "                                     (created_at,\n",
    "                                      '%a %b %d %H:%M:%S +0000 %Y'),\n",
    "                                    '%Y-%m-%d %H:%M:%S')\n",
    "    return newTimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating UDF with function for reshaping timestamp to be applied on the column\n",
    "timestampUDF = udf(lambda x:createTimestamp(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying timestampUDF new on column\n",
    "df_work = df_work.withColumn('Timestamp', timestampUDF(F.col('created_at')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping old columns\n",
    "df_work = df_work.drop('created_at')\n",
    "df_work = df_work.drop('full_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68463b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After cleaning text, remove all rows without alphabetic characters\n",
    "df_work = df_work.filter(F.col('Text').rlike('[a-zA-Z]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b03d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display dataset for inspection\n",
    "df_work.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb33e9",
   "metadata": {},
   "source": [
    "Using RDD for creating index column and returning to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effba461",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting dataframe to RDD with additional rowID (index) column\n",
    "from pyspark.sql.types import LongType, StructField, StructType\n",
    "\n",
    "new_schema = StructType([StructField('rowId',LongType(),True)]\n",
    "                        + df_work.schema.fields)\n",
    "zip_rdd = df_work.rdd.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create map for new RDD\n",
    "new_rdd = zip_rdd.map(lambda args: ([args[1]+1] + list(args[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195cef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rewriting df with new data from RDD\n",
    "df_work = spark.createDataFrame(new_rdd,new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983332a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting dataset\n",
    "df_work.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0781e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract every 10th row because HW doesn't support this number of rows\n",
    "df_work = df_work.where(df_work.rowId%10==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting number of rows\n",
    "df_work.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e291419",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9d5ae",
   "metadata": {},
   "source": [
    "Creating Word clound to inspect top 50 words used. Then extracting compound polarity (sentiment) from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e24a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading text into variable\n",
    "blob = df_work.select('Text').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing text variable to library\n",
    "wc = WordCloud(background_color='white',\n",
    "                    stopwords =  set(STOPWORDS),\n",
    "                    max_words = 50, \n",
    "                    random_state = 42,)\n",
    "wc.generate(' '.join(blob['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating word cloud image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling VADER function for sentimental analysis\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to extract compound sentiment from tweets\n",
    "def polarity(text):\n",
    "    text = analyser.polarity_scores(text)['compound']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sentiment function over UDF\n",
    "polarityUDF = udf(lambda x:polarity(x),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying UDF on tweet text column and storing result in new polarity column\n",
    "df_work = df_work.withColumn('polarity', polarityUDF(F.col('Text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting result of sentiment\n",
    "df_work.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframe to pandas for extensive analysis\n",
    "df_pd = df_work.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15141566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of rows to confirm dataset conversion passed\n",
    "df_pd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42813fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking info for column names and types\n",
    "df_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e401821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking empty rows with empty text just in case\n",
    "df_pd['Text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acaa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging columns\n",
    "df_pd = df_pd.reindex(columns=['rowId', 'Timestamp', 'Text', 'polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataset for column names\n",
    "df_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of neutral sentiment rows\n",
    "df_pd.query('polarity == 0.0000').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd8c1e",
   "metadata": {},
   "source": [
    "Final dataset will have one row per day. Current dataset has more than one tweet per day. To reduce dataset, we will take average of sentiment in one day. Before this calculation neutral sentiment rows will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e150e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting unique timestamps\n",
    "df_pd['Timestamp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing timestamp for only YYYY-MM-DD because we only need days\n",
    "df_pd['Timestamp'] = df_pd['Timestamp'].str.slice(stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3157bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting dataset\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57133d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df that will be used for modeling and predictions\n",
    "df_model = pd.DataFrame(columns=['Timestamp','polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for aggregating non neutral sentiment rows and dividing by number of rows to get average compound sentiment per day\n",
    "a=0\n",
    "b=0\n",
    "c=0\n",
    "for i in df_pd['Timestamp'].unique():\n",
    "    a = df_pd['polarity'][(df_pd['polarity'] != 0.0000) &\n",
    "                          (df_pd['Timestamp'] == i)].sum()\n",
    "    b = len(df_pd[(df_pd['polarity'] != 0.0000) &\n",
    "                  (df_pd['Timestamp'] == i)])\n",
    "    c=a/b\n",
    "    new_row = pd.DataFrame({'Timestamp':i,'polarity':c},index=[0])\n",
    "    df_model = pd.concat([new_row,df_model.loc[:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b332395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index of dataset\n",
    "df_model = df_model[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b232d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting dataset\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting timestamp from string to pandas datetime type\n",
    "df_model['Timestamp'] = pd.to_datetime(df_model['Timestamp'],format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb034478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving timestamp to index of df\n",
    "df_model = df_model.set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing name of the index column\n",
    "df_model.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting dataset\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment is originally [-1,1]. For modeling scaling the data to [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit(df_model)\n",
    "df_model['polarity'] = scaler.transform(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e561b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting positive and negative sentiment\n",
    "df_pd.query('polarity > 0.5').count()\n",
    "df_pd.query('polarity < 0.5').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ba76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting sentiment data\n",
    "plt.ylabel('polarity')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.plot(df_model.index, df_model['polarity'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1de983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposing sentiment\n",
    "result = seasonal_decompose(df_model['polarity'], model='additive')\n",
    "pyplot.rc(\"figure\",figsize=(10,8))\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547883cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "plot_corr(df_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f19b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF plot\n",
    "plot_acf(df_model['polarity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF plot\n",
    "plot_pacf(df_model['polarity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287f89d",
   "metadata": {},
   "source": [
    "The ADF test produces a test statistic that measures the strength of evidence against the null hypothesis of non-stationarity. The test statistic is compared against critical values to determine whether the null hypothesis can be rejected or not. If the test statistic is lower than the critical values, it suggests that the time series is stationary and does not require differencing. Conversely, if the test statistic is higher than the critical values, it indicates that the series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller Test function\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series,autolag='AIC')\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b14e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute ADF test\n",
    "adf_test(df_model['polarity'],title='Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09949c6f",
   "metadata": {},
   "source": [
    "It assesses the stationarity of a time series and provides complementary information to the Augmented Dickey-Fuller (ADF) test. While the ADF test focuses on determining non-stationarity, the KPSS test is used to test for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e361c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kwiatkowski-Phillips-Schmidt-Shin Test\n",
    "result = kpss(df_model['polarity'])\n",
    "print(result)\n",
    "print('KPSS Test Statistics: %.2f' % result[0])\n",
    "print('1%% Critical Value: %.2f' % result[3]['1%'])\n",
    "print('5%% Critical Value: %.2f' % result[3]['5%'])\n",
    "print('10%% Critical Value: %.2f' % result[3]['10%'])\n",
    "print('p-value: %.2f' % result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c7b6f",
   "metadata": {},
   "source": [
    "# Modeling and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c4b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6337d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed4e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d020369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
