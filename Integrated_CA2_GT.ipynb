{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b5cb9b",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e78d2",
   "metadata": {},
   "source": [
    "Importing libraries, starting session and reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac2e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing necessary libraries\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import re\n",
    "from pyspark.sql.types import StringType, TimestampType, FloatType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab16598",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Startig Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Greta\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 500) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Increasing available memory for Spark\n",
    "spark.conf.set(\"spark.sql.legacy.setCommandRejectsSparkCoreConfs\",\"false\")\n",
    "spark.conf.set(\"spark.executor.memory\",\"4g\")\n",
    "spark.conf.set(\"spark.driver.memory\",\"4g\")\n",
    "spark.conf.set(\"spark.driver.maxResultSize\",\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read JSON file into dataframe\n",
    "df = spark.read.json(\"hdfs://localhost:9000/ca2/Greta/greta.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11a764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Printing schema and showing\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83097dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting numer of rows (tweets)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing 1st axis columns\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd522ea",
   "metadata": {},
   "source": [
    "For univariate analysis timestamp and text of tweets are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting only necessary columns and displaying for review\n",
    "df.select(\"created_at\",\"full_text\").show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18abe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Displaying full text for additional review\n",
    "df.select(\"created_at\",\"full_text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06e7ff",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab00d6",
   "metadata": {},
   "source": [
    "Fixing timestamps, cleaning text, reducing data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006be9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating new df from selected columns\n",
    "df_work = df.select(\"created_at\",\"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating function for cleaning texts\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'greta', ' ', text)\n",
    "    text = re.sub(r'thunberg', ' ', text)\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', ' ', text)   \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', ' ', text)   \n",
    "    text = re.sub(r'www.[^ ]+', '', text)  \n",
    "    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', ' ', text)  \n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = [token for token in text.split() if len(token) > 2]\n",
    "    text = [token for token in text if token]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating UDF with function for cleaning text to be applied on the column\n",
    "cleanUDF = udf(lambda x:clean_text(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying cleanUDF on new column\n",
    "df_work = df_work.withColumn('Text', cleanUDF(F.col('full_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating function for reshaping timestamp\n",
    "def createTimestamp(created_at):\n",
    "    newTimestamp = datetime.strftime(datetime.strptime\n",
    "                                     (created_at,\n",
    "                                      '%a %b %d %H:%M:%S +0000 %Y'),\n",
    "                                    '%Y-%m-%d %H:%M:%S')\n",
    "    return newTimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating UDF with function for reshaping timestamp to be applied on the column\n",
    "timestampUDF = udf(lambda x:createTimestamp(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying timestampUDF new on column\n",
    "df_work = df_work.withColumn('Timestamp', timestampUDF(F.col('created_at')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping old columns\n",
    "df_work = df_work.drop('created_at')\n",
    "df_work = df_work.drop('full_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68463b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After cleaning text, remove all rows without alphabetic characters\n",
    "df_work = df_work.filter(F.col('Text').rlike('[a-zA-Z]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b03d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display dataset for inspection\n",
    "df_work.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d04054",
   "metadata": {},
   "source": [
    "Using RDD for creating index column and returning to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effba461",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting dataframe to RDD with additional rowID (index) column\n",
    "from pyspark.sql.types import LongType, StructField, StructType\n",
    "\n",
    "new_schema = StructType([StructField('rowId',LongType(),True)]\n",
    "                        + df_work.schema.fields)\n",
    "zip_rdd = df_work.rdd.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create map for new RDD\n",
    "new_rdd = zip_rdd.map(lambda args: ([args[1]+1] + list(args[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195cef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rewriting df with new data from RDD\n",
    "df_work = spark.createDataFrame(new_rdd,new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983332a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting dataset\n",
    "df_work.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0781e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract every 10th row because HW doesn't support this number of rows\n",
    "df_work = df_work.where(df_work.rowId%10==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting number of rows\n",
    "df_work.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b3bc6",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e24a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = df_work.select('Text').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white',\n",
    "                    stopwords =  set(STOPWORDS),\n",
    "                    max_words = 50, \n",
    "                    random_state = 42,)\n",
    "wc.generate(' '.join(blob['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(text):\n",
    "    text = analyser.polarity_scores(text)['compound']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarityUDF = udf(lambda x:polarity(x),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df_work.withColumn('polarity', polarityUDF(F.col('Text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
